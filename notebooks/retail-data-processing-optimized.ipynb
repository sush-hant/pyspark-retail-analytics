{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "475d5c4e",
   "metadata": {},
   "source": [
    "# Optimized PySpark Retail Data Processing and Sales Analysis\n",
    "\n",
    "This notebook contains an optimized version of the retail data processing pipeline with performance improvements for big data processing (>1M records).\n",
    "\n",
    "- Spark configuration tuning for big data workloads\n",
    "- Strategic caching of reused DataFrames\n",
    "- Combined filtering operations to reduce data passes\n",
    "- Optimal partitioning strategies\n",
    "- Window function optimization with coalesce\n",
    "- Memory management and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460ff201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Configuration for Big Data Optimization\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128MB\")\n",
    "spark.conf.set(\"spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold\", \"0\")\n",
    "\n",
    "print(\"Spark configuration optimized for big data processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql.functions import col, to_date, year, month, round, sum, desc, row_number\n",
    "from pyspark.sql import Window\n",
    "import time\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715a038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance monitoring setup\n",
    "start_time = time.time()\n",
    "print(f\"Starting data processing at: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14314a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with optimal partitioning\n",
    "print(\"Loading raw data...\")\n",
    "df = spark.read.csv(\"/mnt/raw/merge_csv.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df = df.cache()\n",
    "\n",
    "initial_count = df.count()\n",
    "print(f\"Initial dataset size: {initial_count:,} records\")\n",
    "\n",
    "optimal_partitions = max(50, min(200, initial_count // 25000))\n",
    "df = df.repartition(optimal_partitions)\n",
    "print(f\"Repartitioned data into {optimal_partitions} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0284ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined data cleaning operations for optimal performance\n",
    "print(\"Applying data cleaning operations...\")\n",
    "\n",
    "df_cleaned = df.withColumnRenamed('Customer ID', 'CustomerID') \\\n",
    "    .filter(~col('Invoice').startswith('C')) \\\n",
    "    .filter(col('CustomerID').isNotNull()) \\\n",
    "    .filter((col('Quantity') > 0) & (col('Price') > 0)) \\\n",
    "    .dropDuplicates() \\\n",
    "    .cache()  # Cache cleaned data for reuse\n",
    "\n",
    "cleaned_count = df_cleaned.count()\n",
    "print(f\"Cleaned dataset size: {cleaned_count:,} records\")\n",
    "print(f\"Filtered out: {initial_count - cleaned_count:,} records ({((initial_count - cleaned_count) / initial_count * 100):.1f}%)\")\n",
    "\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c36359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined date processing and feature engineering\n",
    "print(\"Processing dates and calculating revenue...\")\n",
    "\n",
    "df_processed = df_cleaned.withColumn('InvoiceDate', to_date(col('InvoiceDate'), 'MM/d/yyyy H:mm')) \\\n",
    "    .withColumn(\"Year\", year(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"Month\", month(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn('Revenue', round(col('Quantity') * col('Price'), 2)) \\\n",
    "    .cache()  # Cache processed data\n",
    "\n",
    "processed_count = df_processed.count()\n",
    "print(f\"Processed dataset size: {processed_count:,} records\")\n",
    "\n",
    "df_cleaned.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd18d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized aggregation with pre-partitioning\n",
    "print(\"Performing monthly sales aggregation...\")\n",
    "\n",
    "monthly_sales = df_processed.repartition(col('Year'), col('Month')) \\\n",
    "    .groupBy('Year','Month','Stockcode','Description') \\\n",
    "    .agg(sum('Revenue').alias('TotalRevenue')) \\\n",
    "    .cache()  # Cache aggregated results\n",
    "\n",
    "monthly_sales_count = monthly_sales.count()\n",
    "print(f\"Monthly sales aggregation complete: {monthly_sales_count:,} product-month combinations\")\n",
    "\n",
    "display(monthly_sales.orderBy(desc('TotalRevenue')).limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046236a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized window function with coalesce\n",
    "print(\"Ranking products by monthly revenue...\")\n",
    "\n",
    "window_spec = Window.partitionBy('Year','Month').orderBy(desc('TotalRevenue'))\n",
    "\n",
    "ranked = monthly_sales.coalesce(50) \\\n",
    "    .withColumn('Rank', row_number().over(window_spec))\n",
    "\n",
    "top10_product = ranked.filter(col('Rank') <= 10).cache()\n",
    "\n",
    "top10_count = top10_product.count()\n",
    "print(f\"Top 10 products analysis complete: {top10_count:,} records\")\n",
    "\n",
    "display(top10_product.orderBy('Year', 'Month', 'Rank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7951c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis: Country sales (optimized)\n",
    "print(\"Performing country sales analysis...\")\n",
    "\n",
    "country_sales = df_processed.repartition(col('Year'), col('Month')) \\\n",
    "    .groupBy('Year','Month','Country') \\\n",
    "    .agg(sum('Revenue').alias('TotalRevenue')) \\\n",
    "    .orderBy(desc('TotalRevenue'))\n",
    "\n",
    "display(country_sales.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ce6978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to Delta Lake with optimized write\n",
    "print(\"Saving results to Delta Lake...\")\n",
    "\n",
    "top10_product.coalesce(10) \\\n",
    "    .write \\\n",
    "    .format('delta') \\\n",
    "    .mode('overwrite') \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .partitionBy('Year','Month') \\\n",
    "    .save('/mnt/processed/top10_product')\n",
    "\n",
    "print(\"Results successfully saved to Delta Lake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013828c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory management and cleanup\n",
    "print(\"Cleaning up cached DataFrames...\")\n",
    "\n",
    "df_processed.unpersist()\n",
    "monthly_sales.unpersist()\n",
    "top10_product.unpersist()\n",
    "\n",
    "print(\"Memory cleanup completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac29a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance summary\n",
    "end_time = time.time()\n",
    "processing_time = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PROCESSING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total processing time: {processing_time:.2f} seconds\")\n",
    "print(f\"Initial dataset size: {initial_count:,} records\")\n",
    "print(f\"Final top 10 products: {top10_count:,} records\")\n",
    "print(f\"Processing rate: {initial_count / processing_time:.0f} records/second\")\n",
    "print(f\"Completed at: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
